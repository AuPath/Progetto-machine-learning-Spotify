\chapter{Campagna sperimentale}
Nota bene viene usat lo stesso seed per il training, in questo si generarno gli sessi folds
s
\section{Approccio}
\subsection{10-folds cross validation}
Per la fase di training dei modelli viene utilizzata la tecnica k-folds cross validation. Si procede quindi creando una partizione del dataset iniziale, dove ogni sottoinsieme, ovvero un fold, ha circa lo stesso numero di istanze ed è più o meno bilanciato tra la classe positiva e negativa.

Per gli esperimenti effettuati, è stato scelto $k = 10$, ovvero il dataset viene diviso casualemnte in 10 parti, utilizzando ad ogni iterazioni una porzione di dati diversa per il trainig-set e test-set. Avendo scelto questo valore di k, la proporzione tra i due insiemi sarà sempre 90\% per il training-set e 10\% per il test-set.

Per ogni modello si vogliono testare diversi iperparametri (\autoref{sec:mode_selection}), pertanto ad ogni iterazione della 10-folds, fissato un fold per il training $\hat{F}$, si esegue un'altra 10-fold cross validation più interna su $\hat{F}$  ($90\%$ del dataset totale), così da poter scegliere l'iperparametro migliore.

Procedendo in questo modo l'ottimizzazione degli iperparametri viene effettuata sfruttando solo il training set e il test set non viene utilizzato nè per il training nè per la scelta degli iperparametri ottimali.

\begin{figure}[H]
	\centering
	\includegraphics[width=13cm]{assets/nested-cv.png}
	\caption{Nested 3-folds cross validation.}
\end{figure}

Una nota importante è che i 10 folds vengono generati in modo casuale, tuttavia per ogni modello di cui si vuole fare il training, questa generazione casuale viene effettuata a partire dallo stesso seed. Questo garatisce di avere folds generati casualmente ma in modo idenctico per ogni modello, in questo modo si ottengono dei risultati statisticamente validi.

\subsection{Model selection}
\label{sec:mode_selection}

\subsubsection{Grid search}
\label{sec:grid-search}
Hyperparameter optimization

\section{Misure di performance}
Le misure di performance sono state ottenute combinando i risultati
ottenuti dalla 10-fold cross validation per ogni fold sia per la
classe positiva che per la classe negativa facendo la macro average
delle misure cercate.

\subsection{Accuracy}
L'accuracy é definita come il numero di veri positivi e veri negativi
rapportati al numero di esempi totali.

Per i modelli esaminati abbiamo ottenuto:

\subsection{Precision}
La precision é definita come il numero di veri positivi rapportati al
numero totale di predizioni positive.

Per i modelli esaminati abbiamo ottenuto:
\subsection{Recall}
La misura di recall é definita come il numero di veri positivi
rapportati al numero di veri positivi e falsi negativi.

Per i modelli esaminati abbiamo ottenuto:
\subsection{F-measure}
La F-Measure é definita come la media armonica di precision e
sensitivity.

Per i modelli esaminati abbiamo ottenuto:
\subsection{Curve ROC e AUC}

\section{Support Vector Machine}
\subsection{Kernel}

\section{Decision Tree} \subsection{Scelta del modello} Come secondo
modello parte del progetto é stato scelto un albero di decisione
(decision tree/classification tree). Questo perché anche con un
dataset relativamente ampio permette il training in un tempo
ragionevole rispetto alla potenza computazionale in nostro possesso.

\subsection{Complexity Parameter}
\subsection{Decision Tree Plot}

\section{Modelli a confronto}
Sono stati messi a confronto i modelli di decision tree, di svm
allenata con kernel radiale e una svm allenata con kernel lineare.

\subsection{Confronto secondo metrica ROC}

\begin{figure}[H]
	\centering
	\includegraphics[width=13cm]{../images/compare_dot_plot_fix.png}
	\caption{Dotplot per metrica ROC.}
	\label{fig:compare_dot_plot}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=13cm]{../images/compare_bw_plot_fix.png}
	\caption{Dotplot per metrica ROC.}
	\label{fig:compare_bw_plot}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=13cm]{../images/compare_splom_plot_fix.png}
	\caption{Scatter plot per metrica ROC.}
	\label{fig:compare_splom_plot}
\end{figure}

\subsection{Confronto Timings} Il modello support vector machine
risulta piu lento per quanto riguarda i tempi di training.

TODO
Tabella con i timings
