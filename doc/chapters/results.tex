\chapter{Campagna sperimentale}

\section{Approccio}
\subsection{10-folds cross validation}
Per la fase di training dei modelli viene utilizzata la tecnica k-folds cross validation. Si procede quindi creando una partizione del dataset iniziale, dove ogni sottoinsieme, ovvero un fold, ha circa lo stesso numero di istanze ed è più o meno bilanciato tra la classe positiva e negativa.

Per gli esperimenti effettuati, è stato scelto $k = 10$, ovvero il dataset viene diviso casualemnte in 10 parti, utilizzando ad ogni iterazioni una porzione di dati diversa per il trainig-set e test-set. Avendo scelto questo valore di k, la proporzione tra i due insiemi sarà sempre 90\% per il training-set e 10\% per il test-set.

Per ogni modello si vogliono testare diversi iperparametri (\autoref{sec:mode_selection}), pertanto ad ogni iterazione della 10-folds, fissato un fold per il training $\hat{F}$, si esegue un'altra 10-fold cross validation più interna su $\hat{F}$  ($90\%$ del dataset totale), così da poter scegliere l'iperparametro migliore.

Procedendo in questo modo l'ottimizzazione degli iperparametri viene effettuata sfruttando solo il training set e il test set non viene utilizzato nè per il training nè per la scelta degli iperparametri ottimali.

\begin{figure}[H]
	\centering
	\includegraphics[width=13cm]{assets/nested-cv.png}
	\caption{Nested 3-folds cross validation.}
\end{figure}

Una nota importante è che i 10 folds vengono generati in modo casuale, tuttavia per ogni modello di cui si vuole fare il training, questa generazione casuale viene effettuata a per mezzo dello stesso seed. Questo garatisce di avere folds generati casualmente ma in modo idenctico per ogni modello, in questo modo si ottengono dei risultati statisticamente validi.

\subsection{Model selection}
\label{sec:mode_selection}
Come abbiamo sopra discusso, per ogni modello viene effettuato il tuning degli iperparametri usando la tecnica 10-folds cross validation.

\subsubsection{Grid search}
\label{sec:grid-search}
Per determinare gli iperparametri ottimali, vengono scelti un insieme di parametri da utilizzare per la fase di training dei modelli. Per ogni modello si considerando tutte le combinazioni dei valori scelti per i parametri.


TODO: I parametri di cp non sono veri.
\begin{table}[H]
	
	\begin{center}
		
		\begin{tabular}{| l | l | l | l |}
			\hline
			\multicolumn{2}{|c|}{\textbf{SVM -  RBF kernel}} &
			\multicolumn{1}{|c|}{\textbf{SVM - Linear kernel}} &
			\multicolumn{1}{c|}{\textbf{Decision tree}}\\
			\hline
			\hline
			\multicolumn{1}{|c}{$C$} &
			\multicolumn{1}{c|}{$sigma$} &
			\multicolumn{1}{c|}{$C$ } &
			\multicolumn{1}{c|}{$cp$}\\
			\hline
			
			$0.001$	 & $0.00001$    & $0.000001$ & $0.01$\\
			$0.01$	  & $0.0001$      &$0.00001$	& $0.02$\\
			$0.1$	   & $0.001$      	&$0.0001$	   & $0.03$\\
			$1$			& $0.01$      	  &$0.001$		  & $0.04$\\
			$10$	   & $0.1$      	  &$0.01$		   & $0.05$\\
				   		   &       	  &$0.1$							   & $0.06$\\
				   	 	   &       	  &$1$									& $0.07$\\
				  		   &       	  &$10$								   & $0.08$\\
						   &       	  &										  & $0.09$\\
						   &       	  & 									  & $0.10$\\
			\hline
		\end{tabular}
		
	\end{center}
	\caption{Tabella iperparametri utilizzati per grid search.}
\end{table}


\section{Misure di performance}
Le misure di performance sono state ottenute combinando i risultati
ottenuti dalla 10-fold cross validation per ogni fold sia per la
classe positiva che per la classe negativa facendo la macro average
delle misure cercate.

\subsection{Accuracy}
L'accuracy é definita come il numero di veri positivi e veri negativi
rapportati al numero di esempi totali.

Per i modelli esaminati abbiamo ottenuto:

\subsection{Precision}
La precision é definita come il numero di veri positivi rapportati al
numero totale di predizioni positive.

Per i modelli esaminati abbiamo ottenuto:
\subsection{Recall}
La misura di recall é definita come il numero di veri positivi
rapportati al numero di veri positivi e falsi negativi.

Per i modelli esaminati abbiamo ottenuto:
\subsection{F-measure}
La F-Measure é definita come la media armonica di precision e
sensitivity.

Per i modelli esaminati abbiamo ottenuto:
\subsection{Curve ROC e AUC}

\section{Support Vector Machine}
\subsection{Kernel}

\section{Decision Tree} \subsection{Scelta del modello} Come secondo
modello parte del progetto é stato scelto un albero di decisione
(decision tree/classification tree). Questo perché anche con un
dataset relativamente ampio permette il training in un tempo
ragionevole rispetto alla potenza computazionale in nostro possesso.

\subsection{Complexity Parameter}
\subsection{Decision Tree Plot}

\section{Modelli a confronto}
Sono stati messi a confronto i modelli di decision tree, di svm
allenata con kernel radiale e una svm allenata con kernel lineare.

\subsection{Confronto secondo metrica ROC}

\begin{figure}[H]
	\centering
	\includegraphics[width=13cm]{../images/compare_dot_plot_fix.png}
	\caption{Dotplot per metrica ROC.}
	\label{fig:compare_dot_plot}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=13cm]{../images/compare_bw_plot_fix.png}
	\caption{Dotplot per metrica ROC.}
	\label{fig:compare_bw_plot}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=13cm]{../images/compare_splom_plot_fix.png}
	\caption{Scatter plot per metrica ROC.}
	\label{fig:compare_splom_plot}
\end{figure}

\subsection{Confronto Timings} Il modello support vector machine
risulta piu lento per quanto riguarda i tempi di training.

TODO
Tabella con i timings
